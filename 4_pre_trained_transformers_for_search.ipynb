{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers for Search\n",
    "\n",
    "Let's take a look at how we can use language models to perform a simple search task - given 50,000 movie reviews, can we find one that describes a film that we might be interested in watching?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We will lean heavily on our `modelling` package for loading pre-trained models and adapting them to the search task, for which we will also need to import PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from modelling import data, utils\n",
    "from modelling import transformer as tfr\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "There are three key parameters we need to set:\n",
    "\n",
    "- The number of text tokens from each review to use for creating embeddings (set to 100 as this matches what we used for training the model).\n",
    "\n",
    "- The name of the pre-trained transformer model to use as the foundation for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 100\n",
    "MODEL_NAME = \"decoder_next_word_gen\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data\n",
    "\n",
    "Load the reviews into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget what I said about Emeril. Rachael Ray is the most irritating personality on the\n",
      "Food Network AND all of television. If you've never seen 30 Minute Meals, then you cannot\n",
      "possibly begin to comprehend how unfathomably annoying she is. I really truly meant that\n",
      "you can't even begin to be boggled by her until you've viewed the show once or twice, and\n",
      "even then all words and intelligent thoughts will fail you. The problem is mostly with\n",
      "her mannerisms as you might have guessed. Ray has a goofy mouth and often imitates the\n",
      "parrot. If you love something or think it's \"awesome\" (a word she uses roughly 87 times\n",
      "per telecast) just say it. And she's constantly using horrible, unfunny catchphrases like\n",
      "\"EVOO\" (Extra virgin olive oil!). SHUT UP! What's worse is Ray has TWO other shows on the\n",
      "network! I think this is some elaborate conspiracy by the terrorists to drive us mad.\n",
      "Give me more Tyler Florence! Ray is lame.\n"
     ]
    }
   ],
   "source": [
    "reviews = data.get_data()[\"review\"].tolist()\n",
    "review = reviews[0]\n",
    "\n",
    "utils.print_wrapped(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Tokenizer\n",
    "\n",
    "We will need a tokenizer to convert reviews from strings to lists of integers, that the model has been trained to use as inputs. We will need to use the same tokenizer as that used for training the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[831, 49, 11, 300, 44, 1, 3, 10505, 1363, 8]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = data.IMDBTokenizer(reviews, 10)\n",
    "\n",
    "tokenizer(review)[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-Trained Model\n",
    "\n",
    "We load a pre-trained transformer decoder (generative) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading .models/decoder_next_word_gen/trained@2023-10-11T03:57:36;loss=5_0740.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NextWordPredictionTransformer(\n",
       "  (_position_encoder): PositionalEncoding(\n",
       "    (_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (_embedding): Embedding(69014, 256)\n",
       "  (_decoder): TransformerDecoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (multihead_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    (dropout3): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (_linear): Linear(in_features=256, out_features=69014, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trained_model: tfr.NextWordPredictionTransformer = utils.load_model(MODEL_NAME)\n",
    "pre_trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapt Pre-Trained Model to Create Document Embeddings\n",
    "\n",
    "Recall that the pre-trained model was original trained to predict the next token in a sequence, which is ultimately a classification task that necessitated the final layer of the model being a linear layer that output the logits for all possible tokens (~30k). \n",
    "\n",
    "Well, we don't need this linear layer to create embeddings - we'd prefer to have the intermediate output from the core transformer block - i.e., the context-aware token embeddings output the from multi-head attention mechanism.\n",
    "\n",
    "To get at these we define a new model (using inheritance) that will only initialise and use the layers of the pre-trained model that we want. We also add an additional step that will map many context-aware embeddings to a single embedding for a whole chunk of text (or document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentEmbeddingTransformer(tfr.NextWordPredictionTransformer):\n",
    "    \"\"\"Adapting a generative model to yield text embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, pre_trained_model: tfr.NextWordPredictionTransformer):\n",
    "        super().__init__(\n",
    "            pre_trained_model._size_vocab,\n",
    "            pre_trained_model._size_embed,\n",
    "            pre_trained_model._n_heads,\n",
    "        )\n",
    "        del self._linear\n",
    "        self.load_state_dict(pre_trained_model.state_dict(), strict=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_causal_mask, x_padding_mask = self._make_mask(x)\n",
    "        out = self._embedding(x) * math.sqrt(torch.tensor(self._size_embed))\n",
    "        out = self._position_encoder(out)\n",
    "        out = self._decoder(\n",
    "            out,\n",
    "            out,\n",
    "            tgt_mask=x_causal_mask,\n",
    "            tgt_key_padding_mask=x_padding_mask,\n",
    "            memory_mask=x_causal_mask,\n",
    "            memory_key_padding_mask=x_padding_mask,\n",
    "        )\n",
    "        out = torch.sum(out.squeeze(), dim=0)\n",
    "        out /= out.norm()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an instance of our document embedding model and feed it a tokenised chunk of text to make sure that what we get back is a single vector with the same dimension as our context-aware embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model = DocumentEmbeddingTransformer(pre_trained_model)\n",
    "embedding = embedding_model(torch.tensor([tokenizer(review)]))\n",
    "embedding.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Reviews using Embeddings\n",
    "\n",
    "We now use our document embedding model to produce an embedding vector for each review in the dataset - all 50,000!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_db = []\n",
    "errors = []\n",
    "\n",
    "embedding_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, review in enumerate(reviews):\n",
    "        try:\n",
    "            review_tokenized = tokenizer(reviews[i])[:CHUNK_SIZE]\n",
    "            review_embedding = embedding_model(torch.tensor([review_tokenized]))\n",
    "            embeddings_db.append(review_embedding)\n",
    "        except Exception:\n",
    "            errors.append(str(i))\n",
    "\n",
    "if errors:\n",
    "    print(f\"ERRORS: {', '.join(errors)}\")\n",
    "\n",
    "embeddings_db = torch.stack(embeddings_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Reviews\n",
    "\n",
    "We now have everything we need approach our search task. We start by specifying a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Classic horror movie that is terrifying\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create an embedding for our query and use [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) to score all reviews for relevance to our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = embedding_model(torch.tensor([tokenizer(query)]))\n",
    "query_results = F.cosine_similarity(query_embedding, embeddings_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the top query result to see if it sounds like a relavent match for our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[review #17991; score = 0.7248]\n",
      "\n",
      "Halloween is not only the godfather of all slasher movies but the greatest horror movie\n",
      "ever! John Carpenter and Debra Hill created the most suspenseful, creepy, and terrifying\n",
      "movie of all time with this classic chiller. Michael Myers is such a phenomenal monster\n",
      "in this movie that he inspired scores of imitators, such as Jason Vorhees (Friday the\n",
      "13th), The Miner (My Bloody Valentine), and Charlie Puckett (The Night Brings Charlie).\n",
      "Okay, so I got a little obscure there, but it just goes to show you the impact that this\n",
      "movie had on the entire horror genre. No longer did a monster have to come from King\n",
      "Tut's tomb or from Dr. Frankenstein's lab. He could be created in the cozy little\n",
      "neighborhoods of suburbia. And on The Night He Came Home...Haddonfield, Illinois and the\n",
      "viewers would never be the same. There are many aspects of this movie that make it the\n",
      "crowning jewel of horror movies. First is the setting...it takes place in what appears to\n",
      "be a normal suburban neighborhood. Many of us who grew up in an area such as this can\n",
      "easily identify with the characters. This is the type of neighborhood where you feel\n",
      "safe, but if trouble starts to brew, nobody wants to lift a finger to get involved\n",
      "(especially when a heavy-breathing madman is trying to skewer our young heroine.) Along\n",
      "with the setting, the movie takes place on Halloween!! The scariest night of the year!\n",
      "While most people are carving jack-o-lanterns, Michael Myers is looking to carve up some\n",
      "teenie-boppers. Besides the setting, there is some great acting. Jamie Lee Curtis does a\n",
      "serviceable job as our heroine, Laurie Strode, a goody-two-shoes high-schooler who can\n",
      "never seem to find a date. However, it is Donald Pleasance, as Dr. Sam Loomis, who really\n",
      "steals the show. His portrayal of the good doctor, who knows just what type of evil hides\n",
      "behind the black eyes of Michael Myers and feels compelled to send him to Hell once and\n",
      "for all, is the stuff of horror legend. However, it is the synthesizer score that really\n",
      "drives this picture as it seems to almost put the viewer into the film. Once you hear it,\n",
      "you will never forget it. I also enjoy the grainy feel to this picture. Nowadays, they\n",
      "seem to sharpen up the image of every movie, giving us every possible detail of the\n",
      "monster we are supposed to be afraid of. In Halloween, John Carpenter never really lets\n",
      "us get a complete look at Michael Myers. He always seems like he is a part of the\n",
      "shadows, and, I think that is what makes him so terrifying. There are many scenes where\n",
      "Michael is partly visible as he spies on the young teens (unbeknownst to them), which\n",
      "adds to his creepiness. If you think about, some wacko could be watching you right now\n",
      "and you wouldn't even know it. Unfortunately for our teenagers (and fortunately for us\n",
      "horror fans), when they find Michael, he's not looking for candy on this Halloween\n",
      "night..he's looking for blood. Finally, Michael Myers, himself, is a key element to this\n",
      "movie's effectiveness. His relentless pursuit of Laurie Strode makes him seem like the\n",
      "killer who will never stop. He is the bogeyman that will haunt you for the rest of your\n",
      "life. So,if you have not seen this movie (if there are still some of you out there who\n",
      "haven't, or even if you have), grab some popcorn, turn off every light, pop this into the\n",
      "old DVD and watch in fright. Trick or Treat!\n"
     ]
    }
   ],
   "source": [
    "query_embedding = embedding_model(torch.tensor([tokenizer(query)]))\n",
    "query_results = F.cosine_similarity(query_embedding, embeddings_db)\n",
    "\n",
    "top_hit = query_results.argsort(descending=True)[0]\n",
    "\n",
    "print(f\"[review #{top_hit}; score = {query_results[top_hit]:.4f}]\\n\")\n",
    "utils.print_wrapped(reviews[top_hit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for a toy model!\n",
    "\n",
    "In practice, pre-trained language models are not used for semantic search tasks (i.e., for sentence embeddings), without having first fine-tuned them to ensure that performance for this task has been optimised (given a suitable metric that we have omitted for this demo). This is beyond the scope of this work, but the next logical step to try."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
